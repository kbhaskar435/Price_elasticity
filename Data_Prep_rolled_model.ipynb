{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bambi\n",
    "!pip install arviz\n",
    "!pip install snowflake.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bambi as bmb\n",
    "import arviz as az\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_required(conn_details,table_details,filter_details):\n",
    "    import snowflake.connector\n",
    "    conn = snowflake.connector.connect(\n",
    "    user = conn_details['user'],\n",
    "    password = conn_details['password'],\n",
    "    role = conn_details['role'],\n",
    "    account = conn_details['account'],\n",
    "    warehouse = conn_details['warehouse'],\n",
    "    database = table_details['database'],\n",
    "    schema = table_details['schema'])\n",
    "    cur=conn.cursor()\n",
    "    filter_condition = f\"\"\n",
    "    for key in filter_details.keys():\n",
    "        filter_condition+= key+' = ' + str(filter_details[key]) + ' and '\n",
    "    filter_condition = filter_condition[:-5]\n",
    "    query = f\"SELECT * FROM  {table_details['tablename']} WHERE \"+filter_condition\n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    dat = pd.DataFrame.from_records(iter(cur),columns=[x[0] for x in cur.description])\n",
    "    cur.close()\n",
    "    return dat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_details ={\n",
    "    \"user\": \"RBHASKAR\",\n",
    "    \"password\": \"Extropy360@1\",\n",
    "    \"role\": 'DATABASE_DEV',\n",
    "    'account': \"FHA02592.us-east-1\",\n",
    "    'warehouse': \"COMPUTE_WH\"\n",
    "}\n",
    "table_details = {\n",
    "    \"database\": \"ARB_PRIMANTI\",\n",
    "    \"schema\": \"ETL\",\n",
    "    \"tablename\": '\"VW_PRODUCT_MIX\"'\n",
    "}\n",
    "\n",
    "table_details_1  = {\n",
    "    \"database\": \"ARB_PRIMANTI\",\n",
    "    \"schema\": \"ETL\",\n",
    "    \"tablename\": '\"VW_TRANSACTION_DAILY\"'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basket Elasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM  \"VW_PRODUCT_MIX\" WHERE CLIENT_LOCATION_ID = 1 and REVENUE_CENTER = 'Restaurant'\n",
      "SELECT * FROM  \"VW_TRANSACTION_DAILY\" WHERE CLIENT_LOCATION_ID = 1 and REVENUE_CENTER = 'Restaurant'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbhas\\AppData\\Local\\Temp\\ipykernel_16520\\4022519982.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filter_data['year'] = pd.to_datetime(filter_data['PERIOD']).dt.year\n",
      "C:\\Users\\kbhas\\AppData\\Local\\Temp\\ipykernel_16520\\4022519982.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filter_data['price_per_item'] = filter_data['SALES_NET']/filter_data['QUANTITY']\n"
     ]
    }
   ],
   "source": [
    "Final_Results = pd.DataFrame()\n",
    "Secondary_data = pd.DataFrame()\n",
    "Primary_data = pd.DataFrame()\n",
    "for client in [1,2,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,29,30,31,33,36,37,38,39,40,41,42,43,44,45,46]:\n",
    "    filter_details = {\n",
    "    \"CLIENT_LOCATION_ID\": client,\n",
    "    \"REVENUE_CENTER\": \"'Restaurant'\",}\n",
    "    processed_data = data_required(conn_details,table_details,filter_details)\n",
    "    normalised_data = data_required(conn_details,table_details_1,filter_details)\n",
    "    \n",
    "    filter_data = processed_data[['PERIOD', 'CLIENT_LOCATION_ID', 'MEAL_PERIOD', 'REVENUE_CENTER','QUANTITY', 'SALES_GROSS', 'SALES_NET', 'SALES_DISCOUNT', 'CLIENT_ID','PRODUCT_NAME', 'CATEGORY_1', 'CATEGORY_2_E360','CATEGORY_3_E360', 'CATEGORY_4_E360', 'CATEGORY_5_E360','MENU_CATEGORY']]\n",
    "    filter_data['year'] = pd.to_datetime(filter_data['PERIOD']).dt.year\n",
    "    filter_data['price_per_item'] = filter_data['SALES_NET']/filter_data['QUANTITY']\n",
    "    filter_data = pd.merge(filter_data,normalised_data[['PERIOD','MEAL_PERIOD','TRANS_COUNT']],on=['PERIOD','MEAL_PERIOD'],how='left')\n",
    "    filter_data['QUANTITY_NORMALISED'] = filter_data['QUANTITY']/filter_data['TRANS_COUNT']\n",
    "    # filter_data['QUANTITY_NORMALISED'] = filter_data['QUANTITY']\n",
    "    filter_data['WEEK'] = pd.to_datetime(filter_data['PERIOD']).dt.to_period('W')\n",
    "    filter_data['WEEK'] = filter_data['WEEK'].dt.start_time\n",
    "    filter_data = filter_data[filter_data['SALES_NET']>0]\n",
    "    rolled_data = filter_data.groupby(['WEEK','CLIENT_LOCATION_ID','PRODUCT_NAME','CATEGORY_1', 'CATEGORY_2_E360','CATEGORY_3_E360', 'CATEGORY_4_E360', 'CATEGORY_5_E360','MENU_CATEGORY']).agg(QUANTITY = (\"QUANTITY_NORMALISED\",'sum'),PRICE_PER_ITEM = ('price_per_item','mean')).reset_index()\n",
    "    product_mapping = pd.read_csv(\"C:\\pricing\\Primanti_product_mapping.csv\")\n",
    "    product_mapping.rename(columns={\"Product Name\":\"PRODUCT_NAME\"},inplace=True)\n",
    "    rolled_data = pd.merge(rolled_data,product_mapping[['PRODUCT_NAME','analysis','major_category','minor_category']],on='PRODUCT_NAME')\n",
    "    rolled_data = rolled_data[rolled_data['analysis']=='Considered']\n",
    "    secondary_data = rolled_data.query(\"major_category=='secondary_item'\").groupby(['WEEK','major_category','minor_category']).agg(QUANTITY = ('QUANTITY','sum')).reset_index()\n",
    "    secondary_data['CLIENT_LOCATION_ID'] = rolled_data['CLIENT_LOCATION_ID'].unique()[0]\n",
    "    top_primary_items = rolled_data.query('WEEK>=\"2022-03-08\" and major_category ==\"primary_item\"').groupby(['PRODUCT_NAME']).agg(QUANTITY = ('QUANTITY','sum')).reset_index().sort_values(by='QUANTITY',ascending=False)[:5]\n",
    "    top_primary_items= top_primary_items[top_primary_items['PRODUCT_NAME']!='#NAME?']\n",
    "    primary_price_data = rolled_data.loc[rolled_data['PRODUCT_NAME'].isin(top_primary_items['PRODUCT_NAME'].to_list()),['WEEK','PRODUCT_NAME','PRICE_PER_ITEM']]\n",
    "    primary_price_data = pd.pivot_table(primary_price_data,index=['WEEK'],columns='PRODUCT_NAME').reset_index()\n",
    "    primary_price_data.columns = primary_price_data.columns.droplevel(0)\n",
    "    primary_price_data = primary_price_data.reset_index().rename_axis(None, axis=1)\n",
    "    primary_price_data['CLIENT_LOCATION_ID'] = rolled_data['CLIENT_LOCATION_ID'].unique()[0]\n",
    "    primary_price_data.rename(columns={\"\":\"WEEK\"},inplace=True)\n",
    "    primary_price_data.drop(columns=['index'],inplace=True)\n",
    "    model_data = pd.merge(secondary_data,primary_price_data,how='left',on=['WEEK',\"CLIENT_LOCATION_ID\"])\n",
    "    model_data.dropna(inplace=True)\n",
    "    product_data = model_data.groupby(['minor_category'])\n",
    "    for p,g in product_data:\n",
    "        prod_data = pd.DataFrame(g)    \n",
    "        prod_data.columns = [col.replace(\" \",\"_\") for col in prod_data.columns]\n",
    "        prod_data.columns = [col.replace(\"-\",\"_\") for col in prod_data.columns]\n",
    "        prod_data['QUANTITY'] = np.log((1+prod_data['QUANTITY'])*1000)\n",
    "        req_cols = [col for col in prod_data.columns if col not in ['WEEK','major_category','minor_category','QUANTITY','CLIENT_LOCATION_ID']]\n",
    "        formula1 = 'QUANTITY ~ '\n",
    "        for column in req_cols:\n",
    "            prod_data[column] = np.log(1+prod_data[column])\n",
    "            formula1 = formula1 +column +\" + \"\n",
    "        formula1 = formula1[:-3]\n",
    "        model = bmb.Model(formula=formula1,data=prod_data,family=\"gaussian\")\n",
    "        results = az.summary(model.fit())\n",
    "        results['minor_category'] = p\n",
    "        results['CLIENT_LOCATION_ID'] = rolled_data['CLIENT_LOCATION_ID'].unique()[0]\n",
    "        Final_Results = pd.concat([Final_Results,results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Elasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Results = pd.DataFrame()\n",
    "for client in [1,2,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,29,30,31,33,36,37,38,39,40,41,42,43,44,45,46]:\n",
    "    filter_details = {\n",
    "    \"CLIENT_LOCATION_ID\": client,\n",
    "    \"REVENUE_CENTER\": \"'Restaurant'\",}\n",
    "    processed_data = data_required(conn_details,table_details,filter_details)\n",
    "    normalised_data = data_required(conn_details,table_details_1,filter_details)\n",
    "    filter_data = processed_data[['PERIOD', 'CLIENT_LOCATION_ID', 'MEAL_PERIOD', 'REVENUE_CENTER','QUANTITY', 'SALES_GROSS', 'SALES_NET', 'SALES_DISCOUNT', 'CLIENT_ID','PRODUCT_NAME', 'CATEGORY_1', 'CATEGORY_2_E360','CATEGORY_3_E360', 'CATEGORY_4_E360', 'CATEGORY_5_E360','MENU_CATEGORY']]\n",
    "    filter_data['year'] = pd.to_datetime(filter_data['PERIOD']).dt.year\n",
    "    filter_data['price_per_item'] = filter_data['SALES_NET']/filter_data['QUANTITY']\n",
    "    filter_data = pd.merge(filter_data,normalised_data[['PERIOD','MEAL_PERIOD','TRANS_COUNT']],on=['PERIOD','MEAL_PERIOD'],how='left')\n",
    "    # filter_data['QUANTITY_NORMALISED'] = filter_data['QUANTITY']/filter_data['TRANS_COUNT']\n",
    "    filter_data['QUANTITY_NORMALISED'] = filter_data['QUANTITY']\n",
    "    filter_data['WEEK'] = pd.to_datetime(filter_data['PERIOD']).dt.to_period('W')\n",
    "    filter_data['WEEK'] = filter_data['WEEK'].dt.start_time\n",
    "    filter_data = filter_data[filter_data['SALES_NET']>0]\n",
    "    rolled_data = filter_data.groupby(['WEEK','CLIENT_LOCATION_ID','PRODUCT_NAME','CATEGORY_1', 'CATEGORY_2_E360','CATEGORY_3_E360', 'CATEGORY_4_E360', 'CATEGORY_5_E360','MENU_CATEGORY']).agg(QUANTITY = (\"QUANTITY_NORMALISED\",'sum'),PRICE_PER_ITEM = ('price_per_item','mean')).reset_index()\n",
    "    product_mapping = pd.read_csv(\"C:\\pricing\\Primanti_product_mapping.csv\")\n",
    "    product_mapping.rename(columns={\"Product Name\":\"PRODUCT_NAME\"},inplace=True)\n",
    "    rolled_data = pd.merge(rolled_data,product_mapping[['PRODUCT_NAME','analysis','major_category','minor_category']],on='PRODUCT_NAME')\n",
    "    rolled_data = rolled_data[rolled_data['analysis']=='Considered']\n",
    "    secondary_data = rolled_data.query(\"major_category=='secondary_item'\").groupby(['WEEK','major_category','minor_category']).agg(QUANTITY = ('QUANTITY','sum')).reset_index()\n",
    "    primary_data = rolled_data.query(\"major_category=='primary_item'\").groupby(['WEEK','major_category','minor_category']).agg(PRICE_PER_ITEM = ('PRICE_PER_ITEM','mean')).reset_index()\n",
    "    primary_price_data = pd.pivot_table(primary_data,index=['WEEK'],columns='minor_category').reset_index()\n",
    "    primary_price_data.columns = primary_price_data.columns.droplevel(0)\n",
    "    primary_price_data = primary_price_data.reset_index().rename_axis(None, axis=1)\n",
    "    primary_price_data.rename(columns={\"\":\"WEEK\"},inplace=True)\n",
    "    primary_price_data.drop(columns=['index'],inplace=True)\n",
    "    model_data = pd.merge(secondary_data,primary_price_data,how='left',on='WEEK')\n",
    "    model_data.dropna(inplace=True)\n",
    "    product_data = model_data.groupby(['minor_category'])\n",
    "    for p,g in product_data:\n",
    "        prod_data = pd.DataFrame(g)\n",
    "        prod_data['QUANTITY'] = np.log((1+prod_data['QUANTITY'])*1000)        \n",
    "        for column in prod_data.columns:\n",
    "          if column in ['pizza','salad','sandwiches']:\n",
    "            prod_data[column] = np.log(1+prod_data[column])\n",
    "            formula12 = 'QUANTITY ~ '+column\n",
    "            print(formula12)\n",
    "            model = bmb.Model(formula=formula12,data=prod_data,family=\"gaussian\")\n",
    "            results = az.summary(model.fit())\n",
    "            results['minor_category'] = p\n",
    "            results['CLIENT_LOCATION_ID'] = rolled_data['CLIENT_LOCATION_ID'].unique()[0]\n",
    "            del formula12\n",
    "            Final_Results = pd.concat([Final_Results,results])\n",
    "          \n",
    "Final_Results.to_csv('/content/Major_Minor_elasticty_Results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4acc314188c1bd4b536b156f0d0952d4fb67821dbb86f356752ab279470272b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
